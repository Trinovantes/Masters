\chapter{Related Works}
\label{chap:related-works}

Writing programs is one method used in computer science education to reinforce and assess practical concepts taught in class. Automating the manual marking process has been a widely studied topic due to the need for quick feedback turnaround and, by removing or minimizing the human element, maximizes objectivity and consistency. This chapter explores several tools in this field; since there are too many tools to count, we will mainly focus on tools designed for interoperability rather than ones designed for niche domains or specific assignments.

%--------------------------------------------------------------------------------
\section{Input/Output Marking}
%--------------------------------------------------------------------------------

The earliest published example we found of automated programming assignment marking dates back to 1989 by Isaacson and Scott \cite{isaacson1989automating}. Their approach is one of the most common automated marking techniques used in practice today: they compile each student's program (if possible), execute the students' programs with input files, and compare the programs' outputs with expected output files. This process is straightforward and easy-to-understand for testing program functionality. There are many subsequent works \cite{cheang2003automated, higgins2003coursemarker, jackson1997grading, morris2003automatic, spacco2006experiences} that further enhance their process. These tools ultimately all follow the same three steps.

%--------------------------------------------------------------------------------
\section{Fill-In-The-Gap Marking}
%--------------------------------------------------------------------------------

Another increasingly common assignment style and marking technique is called \textquote{fill-in-the-gap}. As this name implies, these assignments provide students with gaps in instructor-provided template code to fill prior to compilation and analysis. For example, an instructor might provide students with a method signature for a binary search algorithm and task them to implement the algorithm itself. Lieberman \cite{lieberman1986example} has stated that this assignment style is the most effective way for beginner programmers to apply newly-taught concepts. Due to the rigid nature of these assignment specifications and extremely small analysis space, many static analysis techniques have been developed to analyze these gaps.

The Environment for Learning to Program (ELP) marking platform developed by Truong et al. \cite{truong2004static} is an example of the fill-in-the-gap assignment style used in practice. After a student submits their assignment, their tool compiles the student program and extracts the relevant \textquote{gap} from the resulting AST for later analysis. Similar to our tool, they also perform some AST normalization prior to analysis in order to minimize the amount of variety among functionally-similar programs. After extracting the gaps, they perform analysis such as checking variable states and invariants before/after gaps and metrics such as line-length and cyclomatic complexity.

OverCode developed by Glassman et al. \cite{glassman2015overcode} is another marking platform based on the idea of fill-in-the-gap style Python programming assignments in Massive Open Online Courses (MOOC). They first clean up student programs such as normalizing variable names. They then cluster student solutions based on line-by-line comparison such that programs with similar lines are in the same clusters. The resulting clusters are presented to instructors to manually provide feedback and scores. We note that their approach was mainly made possible due to the simplicity of Python and of the assignment specification as well as the massive amount of submission data available to analyze. In offline classes such as the class we analyzed with only hundreds of students, rather than tens of thousands, it is unlikely their approach will be able to extract meaningful-sized clusters for analysis.

The disadvantage of these types of assignments and their corresponding tools is that they are mostly used in beginner programming classes rather than upper-year classes, such as the one analyzed in this thesis. Furthermore, they do not score student assignments beyond input/output testing; however, it is theoretically possible for them to apply a score based on their static analysis results such as applying deductions for high cyclomatic complexity. Nonetheless, advanced programming assignments such as our Non-Blocking IO and Parallel Processing assignments are more open-ended and generally consist of significantly larger non-divisible methods that are not as easily analyzable by the aforementioned tools.

%--------------------------------------------------------------------------------
\section{AST-Based Marking}
%--------------------------------------------------------------------------------

There are many prior works exploring the idea of automatically comparing student solutions to reference solutions at the solution level using ASTs. One common element found in all of these works is the need for normalizing ASTs by collapsing functionally-similar substructures into canonical forms prior to analysis. This is essential to reducing the number of distinct solutions to analyze and avoiding falsely penalizing functionally-similar code. In addition to the field of automated program marking, normalizing AST substructures is widely studied in other fields such as plagiarism and clone detection \cite{baxter1998clone, feng2013code}.

Singh et al. \cite{singh2013automated} developed an error-correction language for describing steps to transform an incorrect student solution into a correct reference solution. In addition to reference solutions, the instructor must also describe common student errors and the steps to correct them using the error-correction language. Their system then utilizes a constraint solver to find the minimum steps needed to correct the student solutions. Finally, it scores the student based on the steps. This is similar to our tool's cost model described in Section~\ref{sec:cam-cost-model} where we associate a cost for any change to be made to the student AST.

Thorburn and Rowe's PASS tool \cite{thorburn1997pass} also requires additional work from the instructors prior to scoring students. Their tool requires the instructor to specify an hierarchical \textquote{solution plan} for each assignment; this is essentially a breakdown of the steps to solve a problem. For example, a merge sort problem consists of a \textquote{sort} function at the top level; it then consists of sub-components such as checking the base case, pivoting the input, and the making two recursive calls. Their tool attempts to find equivalent program components in the student code. They check for equivalence by comparing component outputs when given randomly generated inputs. Students are then scored based on the number of equivalent components found.

The main drawback for both Singh et al.'s and Thorburn and Rowe's tools is the additional work needed for instructors beyond providing reference solutions. Singh et al.'s work requires instructors to anticipate and describe common errors and corrections in a custom language. This may prove difficult as one cannot anticipate all possible errors a student may make. Thorburn and Rowe's work requires instructors to describe a solution plan by hierarchically breaking down a problem into smaller sub-problems. This may not always be possible for every assignment as some assignments may have large indivisible components.

AssignSim developed by Naud\'e et al. \cite{naude2010marking} was perhaps the most similar prior work to ours. Their tool directly compares solution ASTs and generates a score based on their similarities. Our works differ in how we compare the post-processed ASTs and score the student. They generate scores by directly analyzing the graphs and assigning scores based on the differing nodes and their respective neighbour nodes. On the other hand, our work utilizes a generic tree edit distance algorithm to compare the graphs and then aggregate the various edit distances into a mark. Their results were slightly better than ours when their set of reference solutions contained both high quality and low quality solutions (i.e. both high marks and low marks). However, when they only had high quality solutions, their accuracy fell to similar levels as ours. This led us to speculate that our choice of reference solutions (only high marks) may also had an influence in our results; however since the majority of past students in our data set have received full marks, we do not have sufficient data to pursue further investigation.
